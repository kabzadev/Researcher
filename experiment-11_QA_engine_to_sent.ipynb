{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 11: QA Engine - Hypothesis-Driven Research\n",
        "\n",
        "A **hypothesis-driven** Q&A engine for researching Salient (mental availability) changes.\n",
        "\n",
        "**IMPORTANT NOTE**\n",
        "- Currently, the only question supported is \"Salience fell by 6 points in Q3 2025 for new look, can you help find external reasons for decreased mental availability for fashion & apparel retail category?\"\n",
        "- This is due to context was added manually around Salience and New Look not due to experiment design\n",
        "\n",
        "**Approach:** Like a human researcher, generates hypotheses **separately by category**:\n",
        "1. ðŸŒ **Market/Macro** - Industry-wide trends (NOT brand-specific)\n",
        "2. ðŸ·ï¸ **Brand** - What the brand did/didn't do\n",
        "3. âš”ï¸ **Competitive** - What competitors are doing\n",
        "\n",
        "**Workflow:**\n",
        "1. **Parse Question** - Extract brand, direction\n",
        "2. **Generate Hypotheses** - Separately for market, brand, competitive\n",
        "3. **Generate Search Queries** - Targeted queries per hypothesis\n",
        "4. **Execute Searches** - Parallel search with Tier 1 source prioritization\n",
        "5. **Return Findings** - Only RELEVANT facts that explain the metric change\n",
        "\n",
        "**Key Rules:**\n",
        "- âœ… Hypotheses separated by category\n",
        "- âœ… Market = industry trends (not brand-specific)\n",
        "- âœ… Only relevant findings (e.g., for decreased Salient, only news that reduces visibility)\n",
        "- âœ… Tier 1 sources prioritized\n",
        "- ðŸš« No inferences - facts only\n",
        "- ðŸš« No vague \"strategy\" news without concrete impact\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ OpenAI client initialized\n",
            "âœ“ Using OpenAI built-in web search with model: gpt-4o-search-preview\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "OPENAI_API_KEY = \"\"\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# ============================================================\n",
        "# OpenAI Web Search Configuration\n",
        "# Using OpenAI's built-in web_search tool via Responses API\n",
        "# ============================================================\n",
        "SEARCH_MODEL = \"gpt-4o-search-preview\"  # Options: \"gpt-4o-search-preview\", \"gpt-4o-mini-search-preview\"\n",
        "\n",
        "print(\"âœ“ OpenAI client initialized\")\n",
        "print(f\"âœ“ Using OpenAI built-in web search with model: {SEARCH_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Data classes defined\n",
            "âœ“ Competitor database loaded (New Look + 15 competitors)\n",
            "âœ“ Metric dictionary loaded: Salient\n",
            "âœ“ Source tiers configured: 12 T1, 15 T2\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Data Classes and Configuration\n",
        "\n",
        "@dataclass\n",
        "class ParsedQuestion:\n",
        "    \"\"\"Structured representation of the user's question.\"\"\"\n",
        "    original_question: str\n",
        "    brand: str\n",
        "    metrics: List[str]\n",
        "    direction: str  # 'increase', 'decrease', or 'change'\n",
        "    time_period: Optional[str] = None\n",
        "    additional_context: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"A single search result with source tracking.\"\"\"\n",
        "    title: str\n",
        "    url: str\n",
        "    snippet: str\n",
        "    source_name: str\n",
        "    date: Optional[str] = None\n",
        "    relevance_score: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class DriverInsight:\n",
        "    \"\"\"An insight about a potential driver with source citation.\"\"\"\n",
        "    insight: str\n",
        "    category: str  # 'macro', 'brand', 'competitive'\n",
        "    confidence: str  # 'high', 'medium', 'low'\n",
        "    sources: List[SearchResult]\n",
        "    \n",
        "# ============================================================\n",
        "# Competitor Database\n",
        "# ============================================================\n",
        "COMPETITOR_DATABASE: Dict[str, List[str]] = {\n",
        "    \"new look\": [\n",
        "        \"primark\", \"marks and spencer\", \"m&s\", \"asos\", \"next\", \n",
        "        \"h&m\", \"shein\", \"zara\", \"river island\", \"boohoo\", \n",
        "        \"very\", \"amazon\", \"tk maxx\", \"george by asda\", \"jd sports\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Metric Dictionary - Salient\n",
        "# ============================================================\n",
        "# Simplified metric dictionary - only definition and interpretation\n",
        "METRIC_DICTIONARY: Dict[str, Any] = {\n",
        "    \"salient\": {\n",
        "        \"name\": \"Salient\",\n",
        "        \"definition\": \"\"\"Salient measures how easily and quickly a brand comes to mind in buying or usage situations. \n",
        "It captures mental availability, not liking or differentiation. A brand is Salient when it is easily recalled, \n",
        "top-of-mind, and mentally linked to category entry points and occasions.\n",
        "\n",
        "Key question: \"How quickly does this brand come to mind when people think about the category?\"\n",
        "\n",
        "What Salient is NOT:\n",
        "- NOT brand awareness alone\n",
        "- NOT differentiation or uniqueness  \n",
        "- NOT needs fulfilment (Meaningful)\n",
        "- NOT usage, loyalty, or satisfaction\n",
        "- NOT short-term campaign recall\"\"\",\n",
        "        \n",
        "        \"interpretation\": {\n",
        "            \"increase\": \"Improved mental availability and speed to mind\",\n",
        "            \"decrease\": \"Fading mental presence or recall\",\n",
        "            \"stable\": \"Stable brand salience (no significant change)\"\n",
        "        },\n",
        "        \n",
        "        \"drivers_of_change\": \"\"\"\n",
        "WHAT INCREASES SALIENCE:\n",
        "- Heavy advertising (especially broad-reach: TV, OOH, digital display)\n",
        "- Strong physical store presence (high footfall locations)\n",
        "- Frequent media mentions and PR coverage\n",
        "- Viral moments, celebrity associations\n",
        "- Distinctive brand assets consistently reinforced\n",
        "\n",
        "WHAT DECREASES SALIENCE (especially for high-street retail brands):\n",
        "- Reduced advertising spend\n",
        "- Store closures or reduced physical presence\n",
        "- Shift to online shopping = less passive brand exposure from walking past stores\n",
        "- Competitor campaigns stealing share of voice\n",
        "- Brand going \"quiet\" - less media activity\n",
        "- Economic pressures reducing category attention overall\n",
        "\n",
        "FOR HIGH-STREET FASHION BRANDS LIKE NEW LOOK:\n",
        "- Physical stores are a major source of passive brand exposure\n",
        "- If consumers shop more online, they see fewer physical stores\n",
        "- Online discovery is intent-based (search) vs. browsing (passive exposure)\n",
        "- This means less \"incidental\" encounters with the brand\n",
        "- Brands heavily reliant on high-street presence are vulnerable to online shift\"\"\",\n",
        "        \n",
        "        \"interpretation_guidance\": \"\"\"\n",
        "- High Salient + Low Meaningful: Brand is well known but weakly relevant\n",
        "- Low Salient + High Meaningful: Brand resonates once considered but struggles to enter choice sets\n",
        "- Sustained Salient improvements usually reflect: consistent brand presence, broad-reach communication, reinforcement of distinctive brand assets\n",
        "- Salient is typically the strongest short-term lever of demand\"\"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_metric_context(metric_name: str) -> str:\n",
        "    \"\"\"Get metric context including drivers of change.\"\"\"\n",
        "    metric = METRIC_DICTIONARY.get(metric_name.lower().replace(\" \", \"_\"), {})\n",
        "    if not metric:\n",
        "        return \"\"\n",
        "    \n",
        "    interp = metric.get('interpretation', {})\n",
        "    return f\"\"\"METRIC: {metric.get('name')}\n",
        "\n",
        "DEFINITION: {metric.get('definition')}\n",
        "\n",
        "INTERPRETATION:\n",
        "- If increases: {interp.get('increase')}\n",
        "- If decreases: {interp.get('decrease')}\n",
        "\n",
        "{metric.get('drivers_of_change', '')}\n",
        "\n",
        "{metric.get('interpretation_guidance', '')}\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# Source Tier Configuration\n",
        "# Tier 1 = Premium authoritative sources (higher confidence)\n",
        "# Tier 2 = Other credible sources (supporting evidence)\n",
        "# ============================================================\n",
        "\n",
        "# TIER 1 SOURCES - Premium, authoritative sources\n",
        "TIER_1_SOURCES: List[str] = [\n",
        "    # Financial News\n",
        "    \"bloomberg.com\",\n",
        "    \"ft.com\",           # Financial Times\n",
        "    \"wsj.com\",          # Wall Street Journal\n",
        "    \n",
        "    # Advertising & Marketing Trade\n",
        "    \"adweek.com\",\n",
        "    \"adage.com\",        # Ad Age / Advertising Age\n",
        "    \"thedrum.com\",      # The Drum\n",
        "    \"campaignlive.com\", # Campaign\n",
        "    \"marketingweek.com\",# Marketing Week\n",
        "    \n",
        "    # Market Research & Intelligence\n",
        "    \"kantar.com\",       # Kantar Media\n",
        "    \"mckinsey.com\",     # McKinsey ConsumerWise\n",
        "    \"mintel.com\",       # Mintel\n",
        "    \"euromonitor.com\",  # Euromonitor\n",
        "]\n",
        "\n",
        "# TIER 2 SOURCES - Other credible sources\n",
        "TIER_2_SOURCES: List[str] = [\n",
        "    # Business News\n",
        "    \"reuters.com\",\n",
        "    \"cnbc.com\",\n",
        "    \"businessinsider.com\",\n",
        "    \"forbes.com\",\n",
        "    \"economist.com\",\n",
        "    \n",
        "    # Industry Publications\n",
        "    \"marketwatch.com\",\n",
        "    \"seekingalpha.com\",\n",
        "    \"brandchannel.com\",\n",
        "    \"prnewswire.com\",\n",
        "    \"statista.com\",\n",
        "    \n",
        "    # Consulting\n",
        "    \"bain.com\",\n",
        "    \"bcg.com\",\n",
        "    \"deloitte.com\",\n",
        "    \"pwc.com\",\n",
        "    \"accenture.com\",\n",
        "]\n",
        "\n",
        "# Combined for backward compatibility\n",
        "TRUSTED_SOURCES: Dict[str, List[str]] = {\n",
        "    \"tier1\": TIER_1_SOURCES,\n",
        "    \"tier2\": TIER_2_SOURCES,\n",
        "    \"all\": TIER_1_SOURCES + TIER_2_SOURCES,\n",
        "}\n",
        "\n",
        "def get_source_tier(url: str) -> int:\n",
        "    \"\"\"\n",
        "    Determine the tier of a source based on its URL.\n",
        "    Returns: 1 for Tier 1, 2 for Tier 2, 3 for unknown/other sources\n",
        "    \"\"\"\n",
        "    url_lower = url.lower()\n",
        "    for domain in TIER_1_SOURCES:\n",
        "        if domain in url_lower:\n",
        "            return 1\n",
        "    for domain in TIER_2_SOURCES:\n",
        "        if domain in url_lower:\n",
        "            return 2\n",
        "    return 3  # Unknown/other source\n",
        "\n",
        "print(\"âœ“ Data classes defined\")\n",
        "print(f\"âœ“ Competitor database loaded (New Look + {len(COMPETITOR_DATABASE.get('new look', []))} competitors)\")\n",
        "print(f\"âœ“ Metric dictionary loaded: Salient\")\n",
        "print(f\"âœ“ Source tiers configured: {len(TIER_1_SOURCES)} T1, {len(TIER_2_SOURCES)} T2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Question parsing functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Question Parsing - Extract Brand, Metrics, Direction\n",
        "\n",
        "def parse_user_question(question: str) -> ParsedQuestion:\n",
        "    \"\"\"\n",
        "    Use LLM to extract structured information from the user's question.\n",
        "    \n",
        "    Extracts:\n",
        "    - Brand name\n",
        "    - Metrics mentioned (awareness, consideration, purchase intent, etc.)\n",
        "    - Direction of change (increase/decrease)\n",
        "    - Time period if mentioned\n",
        "    - Any additional context\n",
        "    \"\"\"\n",
        "    \n",
        "    system_prompt = \"\"\"You are an expert at parsing brand research questions.\n",
        "\n",
        "Extract the following from the user's question:\n",
        "1. brand: The brand being discussed (lowercase)\n",
        "2. metrics: List of metrics mentioned (e.g., \"awareness\", \"consideration\", \"purchase intent\", \"brand perception\", \"market share\", \"NPS\", etc.)\n",
        "3. direction: Whether the metric increased (\"increase\"), decreased (\"decrease\"), or changed without specified direction (\"change\")\n",
        "4. time_period: Any time period mentioned (e.g., \"Q3 2025\", \"last 6 months\", \"YoY\")\n",
        "5. additional_context: Any other relevant context from the question\n",
        "\n",
        "Return ONLY valid JSON with these fields. Use null for missing optional fields.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Parse this question: {question}\"}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        parsed = json.loads(response.choices[0].message.content)\n",
        "        return ParsedQuestion(\n",
        "            original_question=question,\n",
        "            brand=parsed.get(\"brand\", \"unknown\"),\n",
        "            metrics=parsed.get(\"metrics\", []),\n",
        "            direction=parsed.get(\"direction\", \"change\"),\n",
        "            time_period=parsed.get(\"time_period\"),\n",
        "            additional_context=parsed.get(\"additional_context\")\n",
        "        )\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Warning: Failed to parse LLM response, using defaults\")\n",
        "        return ParsedQuestion(\n",
        "            original_question=question,\n",
        "            brand=\"unknown\",\n",
        "            metrics=[],\n",
        "            direction=\"change\"\n",
        "        )\n",
        "\n",
        "def get_competitors(brand: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get list of competitors for a brand.\n",
        "    Returns from database if available, otherwise returns empty list.\n",
        "    \"\"\"\n",
        "    brand_lower = brand.lower().strip()\n",
        "    \n",
        "    # Direct match\n",
        "    if brand_lower in COMPETITOR_DATABASE:\n",
        "        return COMPETITOR_DATABASE[brand_lower]\n",
        "    \n",
        "    # Fuzzy match (check if brand is substring)\n",
        "    for key, competitors in COMPETITOR_DATABASE.items():\n",
        "        if brand_lower in key or key in brand_lower:\n",
        "            return competitors\n",
        "    \n",
        "    return []  # No competitors found\n",
        "\n",
        "print(\"âœ“ Question parsing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Query generation function defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Hypothesis-Driven Query Generation\n",
        "# \n",
        "# Approach:\n",
        "# 1. Generate hypotheses SEPARATELY for each category (market, brand, competitor)\n",
        "# 2. Generate targeted search queries for each hypothesis\n",
        "# 3. Ensure relevance to the original question\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Generate Hypotheses by Category\n",
        "# ============================================================\n",
        "\n",
        "def generate_hypotheses(parsed: ParsedQuestion, competitors: List[str]) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Generate hypotheses separately for each category.\n",
        "    Returns dict with keys: market, brand, competitive\n",
        "    \"\"\"\n",
        "    \n",
        "    brand = parsed.brand\n",
        "    direction = parsed.direction\n",
        "    time_period = parsed.time_period or \"recent months\"\n",
        "    metric_context = get_metric_context(\"salient\")\n",
        "    \n",
        "    all_hypotheses = {\"market\": [], \"brand\": [], \"competitive\": []}\n",
        "    \n",
        "    # 1. MARKET/MACRO hypotheses (industry-level, NOT brand-specific)\n",
        "    # Use year for macro trends\n",
        "    macro_time = \"2025\" if time_period and \"202\" in time_period else time_period\n",
        "    \n",
        "    # Direction-specific examples\n",
        "    if direction == \"decrease\":\n",
        "        direction_examples = \"\"\"For DECREASE: factors reducing visibility/attention\n",
        "- Online shift reducing high-street exposure\n",
        "- Economic pressures reducing category spending\n",
        "- Declining consumer attention to fashion\"\"\"\n",
        "    else:\n",
        "        direction_examples = \"\"\"For INCREASE: factors boosting visibility/attention\n",
        "- Category growth increasing brand exposure\n",
        "- Consumer spending increases on fashion\n",
        "- Renewed interest in high-street shopping\"\"\"\n",
        "    \n",
        "    market_prompt = f\"\"\"Generate SHORT hypotheses about UK fashion retail trends that could {direction.upper()} brand salience.\n",
        "\n",
        "TIME: {macro_time}\n",
        "DIRECTION: {direction.upper()}\n",
        "\n",
        "âš ï¸ DIRECTION CONSTRAINT:\n",
        "ONLY hypotheses that could cause salience to {direction.upper()}.\n",
        "{direction_examples}\n",
        "DO NOT include factors causing the OPPOSITE direction.\n",
        "\n",
        "{metric_context}\n",
        "\n",
        "COVERAGE AREAS:\n",
        "- Consumer spending trends\n",
        "- Online vs high-street shift\n",
        "- Economic conditions\n",
        "- Category attention\n",
        "\n",
        "Return JSON:\n",
        "{{\"hypotheses\": [\n",
        "    {{\n",
        "        \"id\": \"M1\",\n",
        "        \"hypothesis\": \"SHORT: max 15 words about factor causing {direction}\",\n",
        "        \"queries\": [\n",
        "            \"UK fashion [specific topic] {macro_time}\",\n",
        "            \"UK retail [related topic] {macro_time}\"\n",
        "        ]\n",
        "    }}\n",
        "]}}\n",
        "\n",
        "RULES:\n",
        "- Hypotheses must be SHORT (max 15 words)\n",
        "- ONLY factors causing {direction.upper()} (not opposite)\n",
        "- 2 targeted queries per hypothesis\n",
        "- Queries include \"{macro_time}\"\n",
        "\n",
        "Generate 4-5 hypotheses.\"\"\"\n",
        "\n",
        "    market_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": market_prompt}, \n",
        "                  {\"role\": \"user\", \"content\": f\"Generate UK fashion retail industry hypotheses for {time_period}.\"}],\n",
        "        temperature=0.4,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        all_hypotheses[\"market\"] = json.loads(market_response.choices[0].message.content).get(\"hypotheses\", [])\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 2. BRAND hypotheses (what the brand did/didn't do)\n",
        "    # Direction-specific examples for brand\n",
        "    if direction == \"decrease\":\n",
        "        brand_direction_examples = \"\"\"For DECREASE:\n",
        "- Reduced advertising spend\n",
        "- Store closures\n",
        "- Less media/PR activity\n",
        "- Pulled campaigns\"\"\"\n",
        "    else:\n",
        "        brand_direction_examples = \"\"\"For INCREASE:\n",
        "- Increased advertising spend\n",
        "- Store openings/expansion\n",
        "- Major marketing campaigns\n",
        "- High-profile partnerships\"\"\"\n",
        "    \n",
        "    brand_prompt = f\"\"\"Generate SHORT hypotheses about {brand}'s actions that could {direction.upper()} salience.\n",
        "\n",
        "TIME: {time_period}\n",
        "DIRECTION: {direction.upper()}\n",
        "\n",
        "âš ï¸ DIRECTION CONSTRAINT:\n",
        "ONLY hypotheses about {brand} actions causing salience to {direction.upper()}.\n",
        "{brand_direction_examples}\n",
        "DO NOT include actions causing the OPPOSITE direction.\n",
        "\n",
        "{metric_context}\n",
        "\n",
        "COVERAGE AREAS:\n",
        "- Advertising spend changes\n",
        "- Store activity (openings/closures)\n",
        "- Marketing campaigns\n",
        "- Media presence\n",
        "- Partnerships/events\n",
        "\n",
        "Return JSON:\n",
        "{{\"hypotheses\": [\n",
        "    {{\n",
        "        \"id\": \"B1\",\n",
        "        \"hypothesis\": \"SHORT: max 15 words about {brand} action\",\n",
        "        \"queries\": [\n",
        "            \"{brand} [specific action] UK {time_period}\",\n",
        "            \"{brand} [related topic] UK {time_period}\"\n",
        "        ]\n",
        "    }}\n",
        "]}}\n",
        "\n",
        "RULES:\n",
        "- Hypotheses must be SHORT (max 15 words)\n",
        "- ONLY actions causing {direction.upper()} (not opposite)\n",
        "- 2 targeted queries per hypothesis\n",
        "- Queries include \"{time_period}\"\n",
        "\n",
        "Generate 4-5 hypotheses.\"\"\"\n",
        "\n",
        "    brand_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": brand_prompt},\n",
        "                  {\"role\": \"user\", \"content\": f\"Generate hypotheses about {brand}'s actions in {time_period}.\"}],\n",
        "        temperature=0.4,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        all_hypotheses[\"brand\"] = json.loads(brand_response.choices[0].message.content).get(\"hypotheses\", [])\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 3. COMPETITIVE hypotheses (what competitors are doing)\n",
        "    # Direction-specific examples for competitors\n",
        "    if direction == \"decrease\":\n",
        "        comp_direction_examples = \"\"\"For DECREASE (competitor OVERSHADOWS the brand):\n",
        "- Competitor launched major ad campaign (steals share of voice)\n",
        "- Competitor gained significant media coverage\n",
        "- Competitor viral moment or celebrity partnership\"\"\"\n",
        "    else:\n",
        "        comp_direction_examples = \"\"\"For INCREASE (competitor struggles help the brand):\n",
        "- Competitor reduced advertising\n",
        "- Competitor store closures\n",
        "- Competitor negative press/PR issues\"\"\"\n",
        "    \n",
        "    comp_prompt = f\"\"\"Generate SHORT hypotheses about competitor actions affecting {brand}'s salience in the UK.\n",
        "\n",
        "TIME: {time_period}\n",
        "DIRECTION: {brand}'s salience {direction.upper()}\n",
        "COMPETITORS: {', '.join(competitors[:8])}\n",
        "GEOGRAPHY: UK (same market as {brand})\n",
        "\n",
        "âš ï¸ DIRECTION CONSTRAINT:\n",
        "ONLY hypotheses about competitor actions causing {brand}'s salience to {direction.upper()}.\n",
        "{comp_direction_examples}\n",
        "DO NOT include actions causing the OPPOSITE direction.\n",
        "\n",
        "âš ï¸ GEOGRAPHIC CONSTRAINT:\n",
        "- Only UK market activities (where {brand} operates)\n",
        "- For M&S: ONLY their UK CLOTHING business, NOT food\n",
        "\n",
        "{metric_context}\n",
        "\n",
        "COVERAGE AREAS:\n",
        "- Competitor advertising campaigns (UK)\n",
        "- Competitor store activity (UK)\n",
        "- Competitor media coverage (UK)\n",
        "- Competitor partnerships/events (UK)\n",
        "\n",
        "Return JSON:\n",
        "{{\"hypotheses\": [\n",
        "    {{\n",
        "        \"id\": \"C1\",\n",
        "        \"hypothesis\": \"SHORT: max 15 words - [Competitor] UK action\",\n",
        "        \"queries\": [\n",
        "            \"[competitor] [action] UK {time_period}\",\n",
        "            \"[competitor] UK fashion [topic] {time_period}\"\n",
        "        ]\n",
        "    }}\n",
        "]}}\n",
        "\n",
        "RULES:\n",
        "- Hypotheses must be SHORT (max 15 words)\n",
        "- ONLY actions causing {direction.upper()} (not opposite)\n",
        "- Name a SPECIFIC competitor\n",
        "- 2 targeted UK-focused queries per hypothesis\n",
        "- Queries include \"UK\" and \"{time_period}\"\n",
        "\n",
        "Generate 4-5 hypotheses.\"\"\"\n",
        "\n",
        "    comp_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": comp_prompt},\n",
        "                  {\"role\": \"user\", \"content\": f\"Generate competitor hypotheses for {time_period}.\"}],\n",
        "        temperature=0.4,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        all_hypotheses[\"competitive\"] = json.loads(comp_response.choices[0].message.content).get(\"hypotheses\", [])\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return all_hypotheses\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Extract Search Queries from Hypotheses\n",
        "# ============================================================\n",
        "\n",
        "def generate_search_queries(hypotheses: Dict[str, List[Dict]], parsed: ParsedQuestion, competitors: List[str]) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract search queries from the hypothesis objects.\n",
        "    Each hypothesis now contains multiple queries (list).\n",
        "    \"\"\"\n",
        "    \n",
        "    queries = {\"macro\": [], \"brand\": [], \"competitive\": []}\n",
        "    \n",
        "    for h in hypotheses.get(\"market\", []):\n",
        "        # Handle both old format (search_query) and new format (queries list)\n",
        "        if \"queries\" in h and isinstance(h[\"queries\"], list):\n",
        "            queries[\"macro\"].extend(h[\"queries\"])\n",
        "        elif \"search_query\" in h:\n",
        "            queries[\"macro\"].append(h[\"search_query\"])\n",
        "    \n",
        "    for h in hypotheses.get(\"brand\", []):\n",
        "        if \"queries\" in h and isinstance(h[\"queries\"], list):\n",
        "            queries[\"brand\"].extend(h[\"queries\"])\n",
        "        elif \"search_query\" in h:\n",
        "            queries[\"brand\"].append(h[\"search_query\"])\n",
        "    \n",
        "    for h in hypotheses.get(\"competitive\", []):\n",
        "        if \"queries\" in h and isinstance(h[\"queries\"], list):\n",
        "            queries[\"competitive\"].extend(h[\"queries\"])\n",
        "        elif \"search_query\" in h:\n",
        "            queries[\"competitive\"].append(h[\"search_query\"])\n",
        "    \n",
        "    return queries\n",
        "\n",
        "print(\"âœ“ Query generation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ OpenAI Web Search functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Online Search Implementation using OpenAI's Built-in Web Search\n",
        "\n",
        "def search_with_openai(query: str, category: str = \"general\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Execute web search using OpenAI's built-in web_search tool.\n",
        "    \n",
        "    Uses the Chat Completions API with gpt-4o-search-preview model.\n",
        "    Returns the response text along with URL citations.\n",
        "    \"\"\"\n",
        "    \n",
        "    trusted_domains = TRUSTED_SOURCES.get(category, [])\n",
        "    \n",
        "    try:\n",
        "        # Use Chat Completions with search-enabled model\n",
        "        response = client.chat.completions.create(\n",
        "            model=SEARCH_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a research assistant for UK FASHION & CLOTHING retail.\n",
        "\n",
        "âš ï¸ CRITICAL TIME CONSTRAINT:\n",
        "â€¢ Search for the MOST RECENT news matching the time period in the query\n",
        "â€¢ If query mentions \"Q3 2025\" or \"2025\", find 2025 news ONLY\n",
        "â€¢ If no 2025 news exists, state \"No news found for this period\" - DO NOT return older news\n",
        "\n",
        "âš ï¸ INDUSTRY CONSTRAINT:\n",
        "â€¢ ONLY fashion/clothing/apparel retail news\n",
        "â€¢ For M&S: ONLY clothing division, NOT food/grocery\n",
        "â€¢ EXCLUDE: supermarket, grocery, food retail\n",
        "\n",
        "PREFERRED SOURCES: bloomberg.com, ft.com, wsj.com, marketingweek.com, thedrum.com\n",
        "\n",
        "ðŸ“‹ RULES:\n",
        "1. Prioritize news from the time period specified in the query\n",
        "2. Include FULL URL for every fact\n",
        "3. If no recent news found, say so - don't substitute old news\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\", \n",
        "                    \"content\": f\"Search for recent news: {query}\"\n",
        "                }\n",
        "            ],\n",
        "            web_search_options={\n",
        "                \"search_context_size\": \"high\"  # Use high for better results\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # Extract the response\n",
        "        message = response.choices[0].message\n",
        "        content = message.content\n",
        "        \n",
        "        # Extract citations from annotations\n",
        "        citations = []\n",
        "        \n",
        "        # Check for annotations (the structure varies by API version)\n",
        "        if hasattr(message, 'annotations') and message.annotations:\n",
        "            for annotation in message.annotations:\n",
        "                try:\n",
        "                    citation_data = {}\n",
        "                    \n",
        "                    # The annotation object might have different structures\n",
        "                    # Try accessing as a dict-like object\n",
        "                    if hasattr(annotation, 'model_dump'):\n",
        "                        ann_dict = annotation.model_dump()\n",
        "                    elif hasattr(annotation, '__dict__'):\n",
        "                        ann_dict = annotation.__dict__\n",
        "                    else:\n",
        "                        ann_dict = {}\n",
        "                    \n",
        "                    # Extract URL and title from various possible locations\n",
        "                    if 'url_citation' in ann_dict:\n",
        "                        url_cit = ann_dict['url_citation']\n",
        "                        citation_data[\"url\"] = url_cit.get('url', '') if isinstance(url_cit, dict) else getattr(url_cit, 'url', '')\n",
        "                        citation_data[\"title\"] = url_cit.get('title', 'Source') if isinstance(url_cit, dict) else getattr(url_cit, 'title', 'Source')\n",
        "                    elif 'url' in ann_dict:\n",
        "                        citation_data[\"url\"] = ann_dict['url']\n",
        "                        citation_data[\"title\"] = ann_dict.get('title', 'Source')\n",
        "                    \n",
        "                    # Get text indices\n",
        "                    citation_data[\"start_index\"] = ann_dict.get('start_index', 0)\n",
        "                    citation_data[\"end_index\"] = ann_dict.get('end_index', 0)\n",
        "                    \n",
        "                    if citation_data.get(\"url\"):\n",
        "                        citations.append(citation_data)\n",
        "                        \n",
        "                except Exception as ann_error:\n",
        "                    # Silently continue on annotation parse errors\n",
        "                    pass\n",
        "        \n",
        "        # Fallback: Extract URLs from text using regex if no/few citations found\n",
        "        if content:\n",
        "            import re\n",
        "            url_pattern = r'https?://[^\\s\\)\\]\\>\\\"\\'<]+' \n",
        "            found_urls = re.findall(url_pattern, content)\n",
        "            existing_urls = [c.get(\"url\", \"\") for c in citations]\n",
        "            \n",
        "            for url in found_urls[:15]:  # Limit to 15 URLs\n",
        "                # Clean URL (remove trailing punctuation)\n",
        "                url = url.rstrip('.,;:!?')\n",
        "                \n",
        "                # Skip social media sources\n",
        "                if is_social_media(url):\n",
        "                    continue\n",
        "                    \n",
        "                if url and url not in existing_urls:\n",
        "                    # Try to extract a title from surrounding text\n",
        "                    url_pos = content.find(url)\n",
        "                    context_start = max(0, url_pos - 100)\n",
        "                    context = content[context_start:url_pos]\n",
        "                    \n",
        "                    citations.append({\n",
        "                        \"url\": url,\n",
        "                        \"title\": \"Source\",\n",
        "                        \"start_index\": url_pos,\n",
        "                        \"end_index\": url_pos + len(url)\n",
        "                    })\n",
        "                    existing_urls.append(url)\n",
        "        \n",
        "        return {\n",
        "            \"text\": content,\n",
        "            \"citations\": citations,\n",
        "            \"category\": category,\n",
        "            \"query\": query\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ OpenAI web search error: {e}\")\n",
        "        return {\n",
        "            \"text\": f\"Search failed for: {query}. Error: {str(e)}\",\n",
        "            \"citations\": [],\n",
        "            \"category\": category,\n",
        "            \"query\": query,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Excluded social media domains\n",
        "EXCLUDED_DOMAINS = [\n",
        "    \"twitter.com\", \"x.com\",\n",
        "    \"facebook.com\", \"fb.com\",\n",
        "    \"instagram.com\",\n",
        "    \"tiktok.com\",\n",
        "    \"linkedin.com\",\n",
        "    \"reddit.com\",\n",
        "    \"pinterest.com\",\n",
        "    \"youtube.com\",  # User-generated content\n",
        "    \"tumblr.com\",\n",
        "]\n",
        "\n",
        "def is_social_media(url: str) -> bool:\n",
        "    \"\"\"Check if a URL is from an excluded social media domain.\"\"\"\n",
        "    url_lower = url.lower()\n",
        "    return any(domain in url_lower for domain in EXCLUDED_DOMAINS)\n",
        "\n",
        "def extract_sources_from_response(response: Dict[str, Any]) -> List[SearchResult]:\n",
        "    \"\"\"\n",
        "    Extract SearchResult objects from OpenAI web search response.\n",
        "    Includes source tier classification and filters out social media sources.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    citations = response.get(\"citations\", [])\n",
        "    text = response.get(\"text\", \"\")\n",
        "    \n",
        "    for citation in citations:\n",
        "        url = citation.get(\"url\", \"\")\n",
        "        title = citation.get(\"title\", \"Unknown\")\n",
        "        \n",
        "        # Skip social media sources\n",
        "        if is_social_media(url):\n",
        "            continue\n",
        "        \n",
        "        # Extract domain from URL\n",
        "        try:\n",
        "            source_domain = url.split(\"/\")[2] if url else \"unknown\"\n",
        "        except:\n",
        "            source_domain = \"unknown\"\n",
        "        \n",
        "        # Determine source tier\n",
        "        tier = get_source_tier(url)\n",
        "        tier_label = f\"[Tier {tier}]\" if tier <= 2 else \"[Other]\"\n",
        "        \n",
        "        # Extract snippet from the text around the citation\n",
        "        start = citation.get(\"start_index\", 0)\n",
        "        end = citation.get(\"end_index\", len(text))\n",
        "        snippet = text[max(0, start-100):min(len(text), end+100)]\n",
        "        \n",
        "        # Higher relevance score for Tier 1 sources\n",
        "        relevance = 1.0 if tier == 1 else (0.8 if tier == 2 else 0.6)\n",
        "        \n",
        "        results.append(SearchResult(\n",
        "            title=f\"{tier_label} {title}\",\n",
        "            url=url,\n",
        "            snippet=snippet[:300] + \"...\" if len(snippet) > 300 else snippet,\n",
        "            source_name=source_domain,\n",
        "            date=None,  # OpenAI doesn't always provide dates\n",
        "            relevance_score=relevance\n",
        "        ))\n",
        "    \n",
        "    # Sort by relevance (Tier 1 first)\n",
        "    results.sort(key=lambda x: x.relevance_score, reverse=True)\n",
        "    \n",
        "    # If no citations but we have text, create a single result from the response\n",
        "    if not results and text:\n",
        "        results.append(SearchResult(\n",
        "            title=f\"[Other] Search result for: {response.get('query', 'query')[:50]}\",\n",
        "            url=\"\",\n",
        "            snippet=text[:500] + \"...\" if len(text) > 500 else text,\n",
        "            source_name=\"openai_web_search\",\n",
        "            date=None,\n",
        "            relevance_score=0.5\n",
        "        ))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def execute_all_searches(queries: Dict[str, List[str]], results_per_query: int = 3) -> Dict[str, List[SearchResult]]:\n",
        "    \"\"\"\n",
        "    Execute all search queries using OpenAI's web search IN PARALLEL for faster execution.\n",
        "    \"\"\"\n",
        "    all_results = {\n",
        "        \"macro\": [],\n",
        "        \"brand\": [],\n",
        "        \"competitive\": []\n",
        "    }\n",
        "    \n",
        "    # Store raw responses for later use in summary generation\n",
        "    all_raw_responses = {\n",
        "        \"macro\": [],\n",
        "        \"brand\": [],\n",
        "        \"competitive\": []\n",
        "    }\n",
        "    \n",
        "    # Flatten all queries with their categories for parallel execution\n",
        "    all_queries = []\n",
        "    for category, query_list in queries.items():\n",
        "        for query in query_list:\n",
        "            all_queries.append((category, query))\n",
        "    \n",
        "    print(f\"\\nðŸš€ Executing {len(all_queries)} searches in PARALLEL...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Execute all searches in parallel using ThreadPoolExecutor\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Submit all search tasks\n",
        "        future_to_query = {\n",
        "            executor.submit(search_with_openai, query, category): (category, query)\n",
        "            for category, query in all_queries\n",
        "        }\n",
        "        \n",
        "        # Collect results as they complete\n",
        "        for future in as_completed(future_to_query):\n",
        "            category, query = future_to_query[future]\n",
        "            try:\n",
        "                response = future.result()\n",
        "                all_raw_responses[category].append(response)\n",
        "                \n",
        "                # Extract structured results\n",
        "                results = extract_sources_from_response(response)\n",
        "                all_results[category].extend(results)\n",
        "                \n",
        "                # Report what was found\n",
        "                citation_count = len(response.get(\"citations\", []))\n",
        "                tier1_count = sum(1 for r in results if \"[Tier 1]\" in r.title)\n",
        "                status = \"âœ“\" if not response.get(\"error\") else \"âš ï¸\"\n",
        "                tier_info = f\"({tier1_count} T1)\" if tier1_count > 0 else \"\"\n",
        "                print(f\"  {status} [{category.upper()}] {query[:50]}... â†’ {citation_count} sources {tier_info}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  âš ï¸ [{category.upper()}] {query[:50]}... â†’ Error: {e}\")\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâ±ï¸ All searches completed in {elapsed:.1f}s (parallel execution)\")\n",
        "    \n",
        "    # Deduplicate by URL\n",
        "    for category in all_results:\n",
        "        seen_urls = set()\n",
        "        unique_results = []\n",
        "        for r in all_results[category]:\n",
        "            if r.url not in seen_urls or r.url == \"\":\n",
        "                if r.url:\n",
        "                    seen_urls.add(r.url)\n",
        "                unique_results.append(r)\n",
        "        all_results[category] = unique_results\n",
        "    \n",
        "    # Store raw responses globally for use in summary\n",
        "    global SEARCH_RAW_RESPONSES\n",
        "    SEARCH_RAW_RESPONSES = all_raw_responses\n",
        "    \n",
        "    # Report tier statistics\n",
        "    print(\"\\nðŸ“Š Source Tier Summary:\")\n",
        "    total_tier1 = 0\n",
        "    total_tier2 = 0\n",
        "    total_other = 0\n",
        "    for category, results in all_results.items():\n",
        "        t1 = sum(1 for r in results if \"[Tier 1]\" in r.title)\n",
        "        t2 = sum(1 for r in results if \"[Tier 2]\" in r.title)\n",
        "        other = len(results) - t1 - t2\n",
        "        total_tier1 += t1\n",
        "        total_tier2 += t2\n",
        "        total_other += other\n",
        "        print(f\"   {category.upper()}: {t1} Tier 1, {t2} Tier 2, {other} Other\")\n",
        "    print(f\"   TOTAL: {total_tier1} Tier 1, {total_tier2} Tier 2, {total_other} Other\")\n",
        "    \n",
        "    if total_tier1 == 0:\n",
        "        print(\"\\nâš ï¸ WARNING: No Tier 1 sources found. Consider refining search queries.\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Global variable to store raw search responses\n",
        "SEARCH_RAW_RESPONSES = {}\n",
        "\n",
        "print(\"âœ“ OpenAI Web Search functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Summary generation function defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Parallel Hypothesis Processing Pipeline\n",
        "# Each hypothesis: Search â†’ Validate â†’ Mini-Summary (in parallel)\n",
        "# Then deterministically combine results\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Process Single Hypothesis (Search â†’ Validate â†’ Summary)\n",
        "# ============================================================\n",
        "\n",
        "def process_single_hypothesis(hypothesis: Dict, category: str, time_period: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process a single hypothesis through the full pipeline:\n",
        "    1. Execute search queries for this hypothesis\n",
        "    2. Validate the hypothesis against search results\n",
        "    3. Generate mini-summary if validated\n",
        "    \n",
        "    Returns the result for this hypothesis.\n",
        "    \"\"\"\n",
        "    \n",
        "    hyp_text = hypothesis.get(\"hypothesis\", \"\")\n",
        "    queries = hypothesis.get(\"queries\", [])\n",
        "    \n",
        "    if not queries:\n",
        "        return {\"status\": \"NO_QUERIES\", \"hypothesis\": hyp_text, \"category\": category}\n",
        "    \n",
        "    # Step 1: Execute searches for this hypothesis\n",
        "    search_results = []\n",
        "    for query in queries[:2]:  # Max 2 queries per hypothesis\n",
        "        try:\n",
        "            result = search_with_openai(query, category)\n",
        "            if result.get(\"text\"):\n",
        "                search_results.append({\n",
        "                    \"query\": query,\n",
        "                    \"text\": result.get(\"text\", \"\"),\n",
        "                    \"citations\": result.get(\"citations\", [])\n",
        "                })\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    \n",
        "    if not search_results:\n",
        "        return {\"status\": \"NO_RESULTS\", \"hypothesis\": hyp_text, \"category\": category}\n",
        "    \n",
        "    # Step 2: Validate this hypothesis against its search results\n",
        "    search_context = \"\\n\\n\".join([f\"Query: {r['query']}\\n{r['text']}\" for r in search_results])\n",
        "    \n",
        "    validation_prompt = f\"\"\"Validate this hypothesis against the search results.\n",
        "\n",
        "HYPOTHESIS: {hyp_text}\n",
        "\n",
        "SEARCH RESULTS:\n",
        "{search_context}\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "    \"status\": \"VALIDATED\" or \"NOT_VALIDATED\",\n",
        "    \"evidence\": \"SHORT factual summary (max 20 words) with key numbers/dates if available\",\n",
        "    \"source_url\": \"URL of the source (if validated)\"\n",
        "}}\n",
        "\n",
        "RULES:\n",
        "- VALIDATED = Search results contain DIRECT evidence supporting this hypothesis\n",
        "- NOT_VALIDATED = No clear evidence found\n",
        "- Evidence must be SHORT: max 20 words, just the key fact with numbers\n",
        "- Good: \"Online sales up 8.3%, in-store up only 0.8% in Q3 2025\"\n",
        "- Bad: Long explanations about consumer behavior shifts...\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": validation_prompt},\n",
        "                {\"role\": \"user\", \"content\": \"Validate this hypothesis.\"}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        \n",
        "        validation = json.loads(response.choices[0].message.content)\n",
        "        \n",
        "        return {\n",
        "            \"status\": validation.get(\"status\", \"NOT_VALIDATED\"),\n",
        "            \"hypothesis\": hyp_text,\n",
        "            \"category\": category,\n",
        "            \"evidence\": validation.get(\"evidence\", \"\"),\n",
        "            \"source_url\": validation.get(\"source_url\", \"\"),\n",
        "            \"search_results\": search_results\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"ERROR\", \"hypothesis\": hyp_text, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Process All Hypotheses in Parallel\n",
        "# ============================================================\n",
        "\n",
        "def process_hypotheses_parallel(hypotheses: Dict[str, List[Dict]], time_period: str) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Process all hypotheses in parallel.\n",
        "    Each hypothesis goes through: Search â†’ Validate â†’ Result\n",
        "    \"\"\"\n",
        "    \n",
        "    results = {\"market\": [], \"brand\": [], \"competitive\": []}\n",
        "    \n",
        "    # Flatten all hypotheses with their categories\n",
        "    all_tasks = []\n",
        "    for cat in [\"market\", \"brand\", \"competitive\"]:\n",
        "        for hyp in hypotheses.get(cat, []):\n",
        "            all_tasks.append((hyp, cat))\n",
        "    \n",
        "    print(f\"   Processing {len(all_tasks)} hypotheses in parallel...\")\n",
        "    \n",
        "    # Process in parallel\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        future_to_hyp = {\n",
        "            executor.submit(process_single_hypothesis, hyp, cat, time_period): (hyp, cat)\n",
        "            for hyp, cat in all_tasks\n",
        "        }\n",
        "        \n",
        "        for future in as_completed(future_to_hyp):\n",
        "            hyp, cat = future_to_hyp[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                results[cat].append(result)\n",
        "                status = \"âœ…\" if result.get(\"status\") == \"VALIDATED\" else \"âŒ\"\n",
        "                print(f\"      {status} {result.get('hypothesis', '')[:50]}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"      âš ï¸ Error: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Combine Results into Final Summary (Deterministic)\n",
        "# ============================================================\n",
        "\n",
        "def combine_results_to_summary(processed_results: Dict[str, List[Dict]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Deterministically combine all processed hypothesis results into final summary.\n",
        "    Only includes validated hypotheses with their evidence.\n",
        "    \"\"\"\n",
        "    \n",
        "    summary = {\n",
        "        \"macro_drivers\": [],\n",
        "        \"brand_drivers\": [],\n",
        "        \"competitive_drivers\": []\n",
        "    }\n",
        "    \n",
        "    validated = {\"market\": [], \"brand\": [], \"competitive\": []}\n",
        "    \n",
        "    # Map categories\n",
        "    category_map = {\n",
        "        \"market\": \"macro_drivers\",\n",
        "        \"brand\": \"brand_drivers\",\n",
        "        \"competitive\": \"competitive_drivers\"\n",
        "    }\n",
        "    \n",
        "    for cat, output_key in category_map.items():\n",
        "        for result in processed_results.get(cat, []):\n",
        "            if result.get(\"status\") == \"VALIDATED\":\n",
        "                # Add to validated list\n",
        "                validated[cat].append({\n",
        "                    \"hypothesis\": result.get(\"hypothesis\", \"\"),\n",
        "                    \"evidence\": result.get(\"evidence\", \"\"),\n",
        "                    \"source_url\": result.get(\"source_url\", \"\")\n",
        "                })\n",
        "                \n",
        "                # Add to summary\n",
        "                summary[output_key].append({\n",
        "                    \"driver\": result.get(\"evidence\", result.get(\"hypothesis\", \"\")),\n",
        "                    \"hypothesis\": result.get(\"hypothesis\", \"\"),\n",
        "                    \"source_urls\": [result.get(\"source_url\", \"\")] if result.get(\"source_url\") else [],\n",
        "                    \"confidence\": \"medium\"\n",
        "                })\n",
        "    \n",
        "    return summary, validated\n",
        "\n",
        "\n",
        "# Legacy functions kept for compatibility\n",
        "def validate_hypotheses(hypotheses, search_results):\n",
        "    \"\"\"Legacy - now handled by process_hypotheses_parallel\"\"\"\n",
        "    return {\"market\": [], \"brand\": [], \"competitive\": []}\n",
        "\n",
        "def generate_summary_from_validated(validated, parsed):\n",
        "    \"\"\"Legacy - now handled by combine_results_to_summary\"\"\"\n",
        "    return {\"macro_drivers\": [], \"brand_drivers\": [], \"competitive_drivers\": []}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LEGACY: Full Summary Generation (kept for reference)\n",
        "# ============================================================\n",
        "\n",
        "def generate_driver_summary(\n",
        "    parsed: ParsedQuestion,\n",
        "    search_results: Dict[str, List[SearchResult]],\n",
        "    competitors: List[str]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generate a comprehensive summary of potential drivers with source citations.\n",
        "    \n",
        "    Uses the raw OpenAI web search responses for richer context,\n",
        "    along with extracted citations for source tracking.\n",
        "    \n",
        "    Returns a structured summary with:\n",
        "    - Key insights by category\n",
        "    - Confidence levels\n",
        "    - Source citations for each claim\n",
        "    \"\"\"\n",
        "    \n",
        "    # Build context from raw search responses (contains full text with inline citations)\n",
        "    context_parts = []\n",
        "    source_index = {}  # Map source ID to full details\n",
        "    source_counter = 1\n",
        "    \n",
        "    # Use raw responses if available (contains richer information)\n",
        "    global SEARCH_RAW_RESPONSES\n",
        "    \n",
        "    for category in [\"macro\", \"brand\", \"competitive\"]:\n",
        "        context_parts.append(f\"\\n{'='*60}\")\n",
        "        context_parts.append(f\"=== {category.upper()} RESEARCH ===\")\n",
        "        context_parts.append(f\"{'='*60}\")\n",
        "        \n",
        "        # Add raw response text (contains OpenAI's search synthesis with citations)\n",
        "        if category in SEARCH_RAW_RESPONSES:\n",
        "            for response in SEARCH_RAW_RESPONSES[category]:\n",
        "                query = response.get(\"query\", \"\")\n",
        "                text = response.get(\"text\", \"\")\n",
        "                citations = response.get(\"citations\", [])\n",
        "                \n",
        "                context_parts.append(f\"\\n--- Query: {query} ---\")\n",
        "                context_parts.append(text)\n",
        "                \n",
        "                # Index the citations\n",
        "                for citation in citations:\n",
        "                    source_id = f\"[{source_counter}]\"\n",
        "                    source_index[source_id] = {\n",
        "                        \"title\": citation.get(\"title\", \"Unknown\"),\n",
        "                        \"url\": citation.get(\"url\", \"\"),\n",
        "                        \"source\": citation.get(\"url\", \"\").split(\"/\")[2] if citation.get(\"url\") else \"unknown\",\n",
        "                        \"date\": None,\n",
        "                        \"category\": category\n",
        "                    }\n",
        "                    source_counter += 1\n",
        "        \n",
        "        # Also add structured results for additional context\n",
        "        if category in search_results:\n",
        "            for r in search_results[category]:\n",
        "                if r.url and r.url not in [s.get(\"url\") for s in source_index.values()]:\n",
        "                    source_id = f\"[{source_counter}]\"\n",
        "                    source_index[source_id] = {\n",
        "                        \"title\": r.title,\n",
        "                        \"url\": r.url,\n",
        "                        \"source\": r.source_name,\n",
        "                        \"date\": r.date,\n",
        "                        \"category\": category\n",
        "                    }\n",
        "                    source_counter += 1\n",
        "    \n",
        "    context = \"\\n\".join(context_parts)\n",
        "    \n",
        "    # Get direction for relevance filtering\n",
        "    direction = parsed.direction\n",
        "    direction_verb = f\"{direction}d\" if direction != 'change' else 'changed'\n",
        "    \n",
        "    system_prompt = f\"\"\"You are a market research analyst compiling factual news for UK FASHION & CLOTHING retail.\n",
        "\n",
        "âš ï¸ CRITICAL - RELEVANCE TO QUESTION:\n",
        "The user is asking about WHY Salient (mental availability) {direction_verb}.\n",
        "ONLY include news items that could PLAUSIBLY EXPLAIN this {direction}.\n",
        "\n",
        "For DECREASED Salient, relevant news includes:\n",
        "âœ… Reduced advertising spend, pulled campaigns\n",
        "âœ… Store closures, reduced physical presence\n",
        "âœ… Negative PR, brand scandals, controversies\n",
        "âœ… Brand going quiet, less media activity\n",
        "âœ… Competitor aggressive campaigns (stealing share of mind)\n",
        "âœ… Industry decline reducing category attention\n",
        "\n",
        "For DECREASED Salient, EXCLUDE:\n",
        "âŒ Positive improvements like \"enhanced omnichannel strategy\" (doesn't explain decrease)\n",
        "âŒ Vague strategy statements without concrete visibility impact\n",
        "âŒ News that doesn't affect brand visibility/awareness\n",
        "\n",
        "ASK YOURSELF: \"Would this news REDUCE how often the brand comes to mind?\"\n",
        "If NO â†’ Don't include it.\n",
        "\n",
        "âš ï¸ INDUSTRY FILTER:\n",
        "â€¢ ONLY fashion/clothing retail news\n",
        "â€¢ For M&S: ONLY clothing division, EXCLUDE food/grocery\n",
        "\n",
        "ðŸš« EXCLUSIONS:\n",
        "1. NO inferences - just facts\n",
        "2. NO social media posts\n",
        "3. NO food/grocery/supermarket news\n",
        "4. NO vague \"strategy\" news without concrete impact\n",
        "\n",
        "SOURCE TIERS & CONFIDENCE:\n",
        "TIER 1: Bloomberg, FT, WSJ, Marketing Week, The Drum, Campaign, Kantar, McKinsey, Mintel\n",
        "TIER 2: Reuters, Forbes, Business Insider, trade publications\n",
        "HIGH = Tier 1 + corroboration | MEDIUM = Single Tier 1 | LOW = No Tier 1\n",
        "\n",
        "Return JSON:\n",
        "{{{{\n",
        "    \"macro_drivers\": [\n",
        "        {{{{\n",
        "            \"driver\": \"Factual statement with specific details (numbers, dates)\",\n",
        "            \"source_urls\": [\"https://...\"],\n",
        "            \"confidence\": \"high/medium/low\"\n",
        "        }}}}\n",
        "    ],\n",
        "    \"brand_drivers\": [\n",
        "        {{{{\n",
        "            \"driver\": \"Factual statement with specific details\",\n",
        "            \"source_urls\": [\"https://...\"],\n",
        "            \"confidence\": \"high/medium/low\"\n",
        "        }}}}\n",
        "    ],\n",
        "    \"competitive_drivers\": [\n",
        "        {{{{\n",
        "            \"driver\": \"Factual statement with specific details\",\n",
        "            \"source_urls\": [\"https://...\"],\n",
        "            \"confidence\": \"high/medium/low\"\n",
        "        }}}}\n",
        "    ]\n",
        "}}}}\n",
        "\n",
        "âš ï¸ OUTPUT RULES:\n",
        "1. JUST STATE THE FACTS - NO interpretations, NO \"why it matters\", NO impact analysis\n",
        "2. Include specific details when available: numbers, percentages, dates\n",
        "3. Good: \"Next reported 10.5% rise in full-price sales in Q3 2025, driven by online strategy\"\n",
        "4. Bad: \"Next's strong performance could steal share of mind\" (this is interpretation)\n",
        "5. Empty arrays preferred over irrelevant/unverified news\n",
        "6. Every item MUST have source URL\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"RESEARCH QUESTION:\n",
        "{parsed.original_question}\n",
        "\n",
        "BRAND: {parsed.brand}\n",
        "METRICS: {', '.join(parsed.metrics)}\n",
        "DIRECTION: {parsed.direction}\n",
        "TIME PERIOD: {parsed.time_period or 'recent'}\n",
        "COMPETITORS: {', '.join(competitors[:5]) if competitors else 'unknown'}\n",
        "\n",
        "SEARCH RESULTS:\n",
        "{context}\n",
        "\n",
        "Analyze these search results and provide a structured summary of potential drivers.\n",
        "Remember: EVERY insight must cite specific sources using [X] notation.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        summary = json.loads(response.choices[0].message.content)\n",
        "        summary[\"source_index\"] = source_index\n",
        "        return summary\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"executive_summary\": \"Error generating summary\",\n",
        "            \"macro_drivers\": [],\n",
        "            \"brand_drivers\": [],\n",
        "            \"competitive_drivers\": [],\n",
        "            \"data_gaps\": [\"Failed to parse LLM response\"],\n",
        "            \"source_index\": source_index\n",
        "        }\n",
        "\n",
        "print(\"âœ“ Summary generation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Display functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Display Functions - Simplified\n",
        "\n",
        "STYLES = \"\"\"\n",
        "<style>\n",
        ".qa-header { text-align: center; margin-bottom: 20px; }\n",
        ".qa-header h1 { font-size: 22px; color: #1e293b; margin: 0; }\n",
        ".subtitle { color: #64748b; font-size: 13px; }\n",
        ".question-card { background: #f1f5f9; padding: 16px; border-radius: 8px; margin-bottom: 16px; border-left: 4px solid #3b82f6; }\n",
        ".parsed-tags { display: flex; flex-wrap: wrap; gap: 6px; margin-top: 8px; }\n",
        ".tag { padding: 4px 10px; border-radius: 12px; font-size: 12px; }\n",
        "\n",
        ".tag.brand { background: #dbeafe; color: #1e40af; }\n",
        ".tag.metric { background: #dcfce7; color: #166534; }\n",
        ".tag.direction-down { background: #fee2e2; color: #991b1b; }\n",
        ".tag.time { background: #f3e8ff; color: #6b21a8; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "def display_parsed_question(parsed: ParsedQuestion, competitors: List[str]):\n",
        "    \"\"\"Display the parsed question with extracted components.\"\"\"\n",
        "    \n",
        "    direction_class = \"direction-up\" if parsed.direction == \"increase\" else \"direction-down\" if parsed.direction == \"decrease\" else \"time\"\n",
        "    direction_icon = \"ðŸ“ˆ\" if parsed.direction == \"increase\" else \"ðŸ“‰\" if parsed.direction == \"decrease\" else \"ðŸ“Š\"\n",
        "    \n",
        "    metrics_tags = \"\".join([f'<span class=\"tag metric\">ðŸ“Š {m}</span>' for m in parsed.metrics])\n",
        "    competitors_text = f\"<p style='margin-top: 12px; color: #64748b; font-size: 13px;'>ðŸ¢ <strong>Competitors:</strong> {', '.join(competitors[:5])}</p>\" if competitors else \"\"\n",
        "    \n",
        "    html = f\"\"\"\n",
        "    <div class=\"question-card\">\n",
        "        <div class=\"question-text\">â“ {parsed.original_question}</div>\n",
        "        <div class=\"parsed-tags\">\n",
        "            <span class=\"tag brand\">ðŸ·ï¸ {parsed.brand.title()}</span>\n",
        "            {metrics_tags}\n",
        "            <span class=\"tag {direction_class}\">{direction_icon} {parsed.direction.title()}</span>\n",
        "            {f'<span class=\"tag time\">ðŸ“… {parsed.time_period}</span>' if parsed.time_period else ''}\n",
        "        </div>\n",
        "        {competitors_text}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "def display_summary(summary: Dict[str, Any]):\n",
        "    \"\"\"Display simple bullet point summary.\"\"\"\n",
        "    \n",
        "    def render_bullets(drivers: List[Dict]) -> str:\n",
        "        if not drivers:\n",
        "            return \"<li style='color:#94a3b8'>No news found</li>\"\n",
        "        \n",
        "        items = []\n",
        "        for d in drivers:\n",
        "            source_urls = d.get('source_urls', [])\n",
        "            # Get first source link\n",
        "            link = \"\"\n",
        "            if source_urls and source_urls[0]:\n",
        "                url = source_urls[0]\n",
        "                try:\n",
        "                    domain = url.split('/')[2]\n",
        "                except:\n",
        "                    domain = \"source\"\n",
        "                link = f' <a href=\"{url}\" target=\"_blank\" style=\"color:#3b82f6;font-size:12px\">[{domain}]</a>'\n",
        "            \n",
        "            items.append(f\"<li><strong>{d.get('driver', '')}</strong>{link}</li>\")\n",
        "        \n",
        "        return \"\".join(items)\n",
        "    \n",
        "    macro = render_bullets(summary.get('macro_drivers', []))\n",
        "    brand = render_bullets(summary.get('brand_drivers', []))\n",
        "    competitive = render_bullets(summary.get('competitive_drivers', []))\n",
        "    \n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family: -apple-system, sans-serif; line-height: 1.6;\">\n",
        "        <h3 style=\"color:#1e40af; border-bottom: 2px solid #3b82f6; padding-bottom: 8px;\">ðŸŒ Market News</h3>\n",
        "        <ul style=\"margin: 12px 0 24px 0;\">{macro}</ul>\n",
        "        \n",
        "        <h3 style=\"color:#059669; border-bottom: 2px solid #10b981; padding-bottom: 8px;\">ðŸ·ï¸ Brand News (New Look)</h3>\n",
        "        <ul style=\"margin: 12px 0 24px 0;\">{brand}</ul>\n",
        "        \n",
        "        <h3 style=\"color:#d97706; border-bottom: 2px solid #f59e0b; padding-bottom: 8px;\">âš”ï¸ Competitor News</h3>\n",
        "        <ul style=\"margin: 12px 0 24px 0;\">{competitive}</ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "print(\"âœ“ Display functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ QA Engine initialized and ready\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Main QA Engine Pipeline\n",
        "\n",
        "class QAEngine:\n",
        "    \"\"\"\n",
        "    QA Engine for analyzing brand metric drivers.\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Parse user question\n",
        "    2. Identify competitors\n",
        "    3. Generate hypotheses (like a human researcher)\n",
        "    4. Generate search queries to test hypotheses\n",
        "    5. Execute searches\n",
        "    6. Generate findings summary\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, client: OpenAI):\n",
        "        self.client = client\n",
        "        \n",
        "    def analyze(self, question: str, progress_callback=None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the full analysis pipeline.\n",
        "        \n",
        "        Args:\n",
        "            question: User's question\n",
        "            progress_callback: Optional function(step, data) to report progress\n",
        "        \"\"\"\n",
        "        \n",
        "        def report(step, data=None):\n",
        "            if progress_callback:\n",
        "                progress_callback(step, data)\n",
        "        \n",
        "        # Step 1: Parse question\n",
        "        report(\"parsing\", None)\n",
        "        parsed = parse_user_question(question)\n",
        "        report(\"parsed\", {\"brand\": parsed.brand, \"direction\": parsed.direction, \"time\": parsed.time_period})\n",
        "        \n",
        "        # Step 2: Get competitors\n",
        "        competitors = get_competitors(parsed.brand)\n",
        "        \n",
        "        # Step 3: Generate hypotheses (by category)\n",
        "        report(\"hypotheses_start\", None)\n",
        "        hypotheses = generate_hypotheses(parsed, competitors)\n",
        "        report(\"hypotheses_done\", hypotheses)\n",
        "        \n",
        "        # Step 4: Generate search queries (for display purposes)\n",
        "        queries = generate_search_queries(hypotheses, parsed, competitors)\n",
        "        report(\"queries_done\", queries)\n",
        "        \n",
        "        # Step 5: Process all hypotheses in PARALLEL\n",
        "        # Each hypothesis: Search â†’ Validate â†’ Result\n",
        "        report(\"processing\", None)\n",
        "        time_period = parsed.time_period or \"2025\"\n",
        "        processed_results = process_hypotheses_parallel(hypotheses, time_period)\n",
        "        report(\"processed\", processed_results)\n",
        "        \n",
        "        # Step 6: Deterministically combine results into final summary\n",
        "        report(\"summarizing\", None)\n",
        "        summary, validated = combine_results_to_summary(processed_results)\n",
        "        report(\"done\", None)\n",
        "        \n",
        "        return {\n",
        "            \"parsed_question\": parsed,\n",
        "            \"competitors\": competitors,\n",
        "            \"hypotheses\": hypotheses,\n",
        "            \"queries\": queries,\n",
        "            \"processed_results\": processed_results,\n",
        "            \"validated_hypotheses\": validated,\n",
        "            \"summary\": summary\n",
        "        }\n",
        "\n",
        "# Initialize the engine\n",
        "qa_engine = QAEngine(client)\n",
        "print(\"âœ“ QA Engine initialized and ready\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ’¬ Interactive Chat\n",
        "\n",
        "Use the chat interface below to ask questions about brand metric changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â¬‡ï¸ Skip to the interactive chat interface below\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Skip - use interactive chat below\n",
        "# Example questions you can ask:\n",
        "# - \"New Look's Salient score fell by 6 points in Q3 2025. What news might explain this?\"\n",
        "# - \"Why might H&M's mental availability have increased?\"\n",
        "# - \"Primark's Salient is declining - what's happening in UK fashion retail?\"\n",
        "\n",
        "print(\"â¬‡ï¸ Skip to the interactive chat interface below\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â¬‡ï¸ Use the chat interface in the next cell\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Skip - use interactive chat below\n",
        "# The chat interface will run the analysis when you submit a question\n",
        "\n",
        "print(\"â¬‡ï¸ Use the chat interface in the next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"font-family:-apple-system,sans-serif; margin-bottom:16px;\">\n",
              "    <h2 style=\"margin:0;\">ðŸ”¬ QA Engine</h2>\n",
              "    <p style=\"color:#64748b; margin:4px 0 0 0; font-size:13px;\">Ask about brand metric changes (Salient, mental availability)</p>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef4e18527019426a81fdeb103b0226ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Textarea(value='', layout=Layout(height='80px', width='100%'), placeholder='Ask about brand metric changes... â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb889bf39c8346e7bf4a72f92fea0a0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(button_style='primary', description='ðŸ” Search', layout=Layout(width='120px'), style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a476a7242514152a0cd19be76d1c879",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cell 11: Interactive Chat Interface\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Create the chat interface\n",
        "output_area = widgets.Output()\n",
        "question_input = widgets.Textarea(\n",
        "    placeholder='Ask about brand metric changes... e.g., \"New Look\\'s Salient score fell by 6 points in Q3 2025. What news might explain this?\"',\n",
        "    layout=widgets.Layout(width='100%', height='80px')\n",
        ")\n",
        "submit_btn = widgets.Button(\n",
        "    description='ðŸ” Search',\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(width='120px')\n",
        ")\n",
        "\n",
        "def format_chat_response(results):\n",
        "    \"\"\"Format results as chat-style HTML with collapsible thinking section.\"\"\"\n",
        "    hypotheses = results.get(\"hypotheses\", {})  # Dict with market, brand, competitive\n",
        "    processed = results.get(\"processed_results\", {})  # Processing results per hypothesis\n",
        "    summary = results.get(\"summary\", {})\n",
        "    \n",
        "    # Build processing results lookup\n",
        "    processed_lookup = {}\n",
        "    for cat, results_list in processed.items():\n",
        "        for r in results_list:\n",
        "            key = r.get(\"hypothesis\", \"\").lower().strip()\n",
        "            processed_lookup[key] = r\n",
        "    \n",
        "    # Build thinking section - show hypotheses with validation status\n",
        "    hyp_items = \"\"\n",
        "    cat_labels = {\"market\": \"ðŸŒ Market\", \"brand\": \"ðŸ·ï¸ Brand\", \"competitive\": \"âš”ï¸ Competitive\"}\n",
        "    for cat, hyps in hypotheses.items():\n",
        "        if hyps:\n",
        "            hyp_items += f\"<div style='margin-top:10px; font-weight:600; color:#475569; border-bottom:1px solid #e2e8f0; padding-bottom:4px;'>{cat_labels.get(cat, cat)}</div>\"\n",
        "            for h in hyps:\n",
        "                hyp_text = h.get('hypothesis', '')\n",
        "                result = processed_lookup.get(hyp_text.lower().strip(), {})\n",
        "                status = result.get(\"status\", \"NOT_VALIDATED\")\n",
        "                is_validated = status == \"VALIDATED\"\n",
        "                status_icon = \"âœ…\" if is_validated else \"âŒ\"\n",
        "                status_color = \"#10b981\" if is_validated else \"#94a3b8\"\n",
        "                hyp_items += f\"<div style='margin:6px 0 2px 8px; color:{status_color};'>{status_icon} {hyp_text}</div>\"\n",
        "    \n",
        "    thinking_html = f\"\"\"\n",
        "    <details style=\"margin:8px 0;\">\n",
        "        <summary style=\"cursor:pointer; color:#64748b; font-size:12px; padding:8px 0;\">\n",
        "            ðŸ’­ Thought for a few seconds...\n",
        "        </summary>\n",
        "        <div style=\"background:#f1f5f9; border-radius:8px; padding:12px; margin-top:8px; max-height:300px; overflow-y:auto; font-size:12px; color:#64748b; line-height:1.5;\">\n",
        "            <div style=\"font-weight:600; margin-bottom:4px;\">Hypotheses (âœ… = validated, âŒ = no evidence):</div>\n",
        "            {hyp_items}\n",
        "        </div>\n",
        "    </details>\n",
        "    \"\"\"\n",
        "    \n",
        "    # Build findings sections - hypothesis-driven format\n",
        "    def format_bullets(drivers):\n",
        "        if not drivers:\n",
        "            return \"<div style='color:#94a3b8; font-style:italic;'>No validated findings</div>\"\n",
        "        items = \"\"\n",
        "        for d in drivers[:5]:\n",
        "            hypothesis = d.get('hypothesis', '')\n",
        "            evidence = d.get('driver', '')\n",
        "            url = d.get('source_urls', [''])[0] if d.get('source_urls') else ''\n",
        "            link = f' <a href=\"{url}\" target=\"_blank\" style=\"color:#3b82f6;font-size:10px\">[source]</a>' if url else ''\n",
        "            \n",
        "            # Bold hypothesis, evidence below (no truncation)\n",
        "            items += f\"\"\"<div style='margin:10px 0;'>\n",
        "                <div style='font-weight:600; color:#1e293b;'>â€¢ {hypothesis}</div>\n",
        "                <div style='margin-left:12px; font-size:12px; color:#64748b;'>â†’ {evidence}{link}</div>\n",
        "            </div>\"\"\"\n",
        "        return items\n",
        "    \n",
        "    macro = format_bullets(summary.get('macro_drivers', []))\n",
        "    brand = format_bullets(summary.get('brand_drivers', []))\n",
        "    competitive = format_bullets(summary.get('competitive_drivers', []))\n",
        "    \n",
        "    return f\"\"\"\n",
        "    <div style=\"background:#f8fafc; border-radius:12px; padding:16px; margin:8px 0; font-family:-apple-system,sans-serif;\">\n",
        "        {thinking_html}\n",
        "        \n",
        "        <div style=\"border-top:1px solid #e2e8f0; padding-top:12px; margin-top:8px;\">\n",
        "            <div style=\"color:#1e40af; font-weight:600; margin-bottom:8px;\">ðŸŒ Market News</div>\n",
        "            <div style=\"font-size:13px; margin-bottom:16px;\">{macro}</div>\n",
        "            \n",
        "            <div style=\"color:#059669; font-weight:600; margin-bottom:8px;\">ðŸ·ï¸ Brand News</div>\n",
        "            <div style=\"font-size:13px; margin-bottom:16px;\">{brand}</div>\n",
        "            \n",
        "            <div style=\"color:#d97706; font-weight:600; margin-bottom:8px;\">âš”ï¸ Competitor News</div>\n",
        "            <div style=\"font-size:13px;\">{competitive}</div>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Progress tracking state\n",
        "progress_state = {\"hypotheses\": {}, \"queries\": {}, \"processed\": {}, \"step\": \"\"}\n",
        "\n",
        "def update_progress_display(question):\n",
        "    \"\"\"Update the display with current progress.\"\"\"\n",
        "    step = progress_state[\"step\"]\n",
        "    hyps = progress_state[\"hypotheses\"]\n",
        "    queries = progress_state[\"queries\"]\n",
        "    processed = progress_state.get(\"processed\", {})\n",
        "    \n",
        "    # Build progress HTML\n",
        "    steps_html = \"\"\n",
        "    step_order = [\"parsing\", \"hypotheses\", \"queries\", \"processing\", \"summarizing\"]\n",
        "    step_labels = {\n",
        "        \"parsing\": \"ðŸ“ Parsing question...\",\n",
        "        \"hypotheses\": \"ðŸ’¡ Generating hypotheses...\",\n",
        "        \"queries\": \"ðŸ”Ž Creating search queries...\",\n",
        "        \"processing\": \"ðŸ”„ Processing hypotheses (search â†’ validate) in parallel...\",\n",
        "        \"summarizing\": \"ðŸ“Š Combining results...\"\n",
        "    }\n",
        "    \n",
        "    for s in step_order:\n",
        "        if step == s:\n",
        "            steps_html += f\"<div style='color:#3b82f6; font-size:13px;'>â³ {step_labels[s]}</div>\"\n",
        "        elif step_order.index(s) < step_order.index(step) if step in step_order else False:\n",
        "            steps_html += f\"<div style='color:#10b981; font-size:12px;'>âœ“ {step_labels[s].split('...')[0]}</div>\"\n",
        "    \n",
        "    # Build hypotheses preview with their queries (if available)\n",
        "    hyp_preview = \"\"\n",
        "    if hyps:\n",
        "        hyp_preview = \"<div style='margin-top:8px; padding:8px; background:#f1f5f9; border-radius:6px; font-size:11px; color:#64748b;'>\"\n",
        "        hyp_preview += \"<div style='font-weight:600; margin-bottom:4px;'>Hypotheses & Queries:</div>\"\n",
        "        for cat, h_list in hyps.items():\n",
        "            if h_list:\n",
        "                hyp_preview += f\"<div style='margin-top:6px; font-weight:500;'>{cat}:</div>\"\n",
        "                for h in h_list[:2]:  # Show first 2 hypotheses per category\n",
        "                    hyp_preview += f\"<div style='margin-left:8px;'>â€¢ {h.get('hypothesis', '')[:40]}...</div>\"\n",
        "                    h_queries = h.get('queries', []) or []\n",
        "                    for q in h_queries[:1]:  # Show first query\n",
        "                        hyp_preview += f\"<div style='margin-left:16px; color:#94a3b8;'>â†’ {q[:50]}...</div>\"\n",
        "        hyp_preview += \"</div>\"\n",
        "    \n",
        "    # No separate query preview needed\n",
        "    query_preview = \"\"\n",
        "    \n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"background:#3b82f6; color:white; border-radius:12px; padding:12px 16px; margin:8px 0; font-family:-apple-system,sans-serif;\">\n",
        "            <strong>You:</strong> {question}\n",
        "        </div>\n",
        "        <div style='padding:12px; font-family:-apple-system,sans-serif;'>\n",
        "            {steps_html}\n",
        "            {hyp_preview}\n",
        "            {query_preview}\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "def on_submit(b):\n",
        "    question = question_input.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "    \n",
        "    # Reset progress state\n",
        "    progress_state[\"hypotheses\"] = {}\n",
        "    progress_state[\"queries\"] = {}\n",
        "    progress_state[\"step\"] = \"parsing\"\n",
        "    \n",
        "    def progress_callback(step, data):\n",
        "        \"\"\"Handle progress updates from the engine.\"\"\"\n",
        "        if step == \"hypotheses_start\":\n",
        "            progress_state[\"step\"] = \"hypotheses\"\n",
        "        elif step == \"hypotheses_done\":\n",
        "            progress_state[\"hypotheses\"] = data or {}\n",
        "            progress_state[\"step\"] = \"queries\"\n",
        "        elif step == \"queries_done\":\n",
        "            progress_state[\"queries\"] = data or {}\n",
        "            progress_state[\"step\"] = \"processing\"\n",
        "        elif step == \"processing\":\n",
        "            progress_state[\"step\"] = \"processing\"\n",
        "        elif step == \"processed\":\n",
        "            progress_state[\"processed\"] = data or {}\n",
        "            progress_state[\"step\"] = \"summarizing\"\n",
        "        elif step == \"summarizing\":\n",
        "            progress_state[\"step\"] = \"summarizing\"\n",
        "        \n",
        "        # Update display\n",
        "        update_progress_display(question)\n",
        "    \n",
        "    update_progress_display(question)\n",
        "    \n",
        "    try:\n",
        "        # Run analysis with progress callback\n",
        "        results = qa_engine.analyze(question, progress_callback=progress_callback)\n",
        "        \n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            # Show question\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"background:#3b82f6; color:white; border-radius:12px; padding:12px 16px; margin:8px 0; font-family:-apple-system,sans-serif;\">\n",
        "                <strong>You:</strong> {question}\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "            \n",
        "            # Show response\n",
        "            display(HTML(format_chat_response(results)))\n",
        "            \n",
        "    except Exception as e:\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"background:#3b82f6; color:white; border-radius:12px; padding:12px 16px; margin:8px 0;\">\n",
        "                <strong>You:</strong> {question}\n",
        "            </div>\n",
        "            <div style='color:#dc2626; padding:12px; background:#fee2e2; border-radius:8px; margin-top:8px;'>\n",
        "                âŒ Error: {str(e)}\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "submit_btn.on_click(on_submit)\n",
        "\n",
        "# Display the chat interface\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"font-family:-apple-system,sans-serif; margin-bottom:16px;\">\n",
        "    <h2 style=\"margin:0;\">ðŸ”¬ QA Engine</h2>\n",
        "    <p style=\"color:#64748b; margin:4px 0 0 0; font-size:13px;\">Ask about brand metric changes (Salient, mental availability)</p>\n",
        "</div>\n",
        "\"\"\"))\n",
        "display(question_input)\n",
        "display(submit_btn)\n",
        "display(output_area)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_experiment8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
